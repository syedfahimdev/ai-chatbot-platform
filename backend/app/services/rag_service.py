"""
RAG (Retrieval-Augmented Generation) service for AI-powered responses.

Generated on: 2024-12-19T10:30:00Z
Generated by syed
"""

import json
import os
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from pathlib import Path

import chromadb
from chromadb.config import Settings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema import Document
import structlog

from app.core.config import settings
from app.models.document import Document as DocumentModel, DocumentAudience

logger = structlog.get_logger()


class RAGService:
    """RAG service for document processing and AI responses."""
    
    def __init__(self):
        """Initialize RAG service with vector database and LLM."""
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        
        # Initialize ChromaDB client with error handling
        try:
            self.chroma_client = chromadb.HttpClient(
                host=settings.CHROMA_HOST,
                port=settings.CHROMA_PORT
            )
            
            # Initialize vector store
            self.vector_store = Chroma(
                client=self.chroma_client,
                collection_name=settings.CHROMA_COLLECTION_NAME,
                embedding_function=self.embeddings
            )
            self.chroma_available = True
        except Exception as e:
            logger.warning(f"ChromaDB connection failed: {e}. RAG features will be limited.")
            self.chroma_client = None
            self.vector_store = None
            self.chroma_available = False
        
        # Initialize LLM
        self.llm = ChatOpenAI(
            model_name=settings.OPENAI_MODEL,
            temperature=settings.OPENAI_TEMPERATURE,
            max_tokens=settings.OPENAI_MAX_TOKENS,
            openai_api_key=settings.OPENAI_API_KEY
        )
        
        # Text splitter for document processing
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        
        # Initialize QA chain
        if self.chroma_available:
            self.qa_chain = self._create_qa_chain()
        else:
            self.qa_chain = None
    
    def _create_qa_chain(self) -> RetrievalQA:
        """Create the QA chain with custom prompt."""
        prompt_template = """You are a helpful AI assistant for a company providing customer support and field service guidance.

Context information from the knowledge base:
{context}

User question: {question}

Please provide a helpful and accurate response based on the context information. If the information is not available in the context, say so clearly. Always cite your sources when possible.

Response:"""
        
        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(
                search_kwargs={"k": 5}
            ),
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True
        )
    
    async def process_document(self, document: DocumentModel, file_path: str) -> bool:
        """Process and index a document in the vector database."""
        try:
            logger.info(f"Processing document: {document.title}")
            
            # Extract text from document
            text_content = await self._extract_text(file_path, document.file_type)
            if not text_content:
                logger.error(f"Failed to extract text from document: {document.title}")
                return False
            
            # Split text into chunks
            text_chunks = self.text_splitter.split_text(text_content)
            
            # Create documents for vector store
            documents = []
            for i, chunk in enumerate(text_chunks):
                doc = Document(
                    page_content=chunk,
                    metadata={
                        "document_id": document.id,
                        "document_title": document.title,
                        "document_audience": document.audience.value,
                        "chunk_index": i,
                        "source": document.filename,
                        "file_type": document.file_type,
                        "uploaded_by": document.uploaded_by,
                        "created_at": document.created_at.isoformat()
                    }
                )
                documents.append(doc)
            
            # Add documents to vector store
            self.vector_store.add_documents(documents)
            
            logger.info(f"Successfully processed document: {document.title}")
            return True
            
        except Exception as e:
            logger.error(f"Error processing document {document.title}: {str(e)}")
            return False
    
    async def _extract_text(self, file_path: str, file_type: str) -> Optional[str]:
        """Extract text content from different file types."""
        try:
            if file_type.lower() == "pdf":
                return await self._extract_pdf_text(file_path)
            elif file_type.lower() == "docx":
                return await self._extract_docx_text(file_path)
            elif file_type.lower() == "txt":
                return await self._extract_txt_text(file_path)
            else:
                logger.error(f"Unsupported file type: {file_type}")
                return None
        except Exception as e:
            logger.error(f"Error extracting text from {file_path}: {str(e)}")
            return None
    
    async def _extract_pdf_text(self, file_path: str) -> str:
        """Extract text from PDF file."""
        import PyPDF2
        
        text = ""
        with open(file_path, "rb") as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        return text
    
    async def _extract_docx_text(self, file_path: str) -> str:
        """Extract text from DOCX file."""
        from docx import Document
        
        doc = Document(file_path)
        text = ""
        for paragraph in doc.paragraphs:
            text += paragraph.text + "\n"
        return text
    
    async def _extract_txt_text(self, file_path: str) -> str:
        """Extract text from TXT file."""
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read()
    
    async def get_response(
        self, 
        question: str, 
        user_role: str,
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """Get AI response with RAG."""
        try:
            # Check if ChromaDB is available
            if not self.chroma_available or not self.qa_chain:
                # Fallback to direct LLM response without RAG
                response = await self.llm.ainvoke(question)
                return {
                    "answer": response.content,
                    "sources": [],
                    "confidence_score": 50,
                    "model_used": settings.OPENAI_MODEL,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "note": "RAG features unavailable - using direct LLM response"
                }
            
            # Filter documents based on user role
            if user_role == "customer":
                # Customers can only access customer and both audience documents
                filter_dict = {
                    "document_audience": {"$in": ["customer", "both"]}
                }
            elif user_role == "field_engineer":
                # Field engineers can access field engineer and both audience documents
                filter_dict = {
                    "document_audience": {"$in": ["field_engineer", "both"]}
                }
            else:
                # Admin can access all documents
                filter_dict = {}
            
            # Update retriever with filter
            self.qa_chain.retriever = self.vector_store.as_retriever(
                search_kwargs={"k": 5, "filter": filter_dict}
            )
            
            # Get response
            result = self.qa_chain({"query": question})
            
            # Extract sources
            sources = []
            if result.get("source_documents"):
                for doc in result["source_documents"]:
                    sources.append({
                        "title": doc.metadata.get("document_title", "Unknown"),
                        "filename": doc.metadata.get("source", "Unknown"),
                        "chunk_index": doc.metadata.get("chunk_index", 0),
                        "audience": doc.metadata.get("document_audience", "Unknown")
                    })
            
            return {
                "answer": result["result"],
                "sources": sources,
                "confidence_score": self._calculate_confidence(result),
                "model_used": settings.OPENAI_MODEL,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error getting RAG response: {str(e)}")
            return {
                "answer": "I apologize, but I encountered an error while processing your request. Please try again later.",
                "sources": [],
                "confidence_score": 0,
                "model_used": settings.OPENAI_MODEL,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
    
    def _calculate_confidence(self, result: Dict[str, Any]) -> int:
        """Calculate confidence score based on response quality."""
        # Simple confidence calculation based on source availability
        if result.get("source_documents"):
            return min(90, 50 + len(result["source_documents"]) * 10)
        return 30
    
    async def delete_document_vectors(self, document_id: int) -> bool:
        """Delete all vectors associated with a document."""
        try:
            # Delete vectors by document_id metadata
            self.vector_store._collection.delete(
                where={"document_id": document_id}
            )
            logger.info(f"Deleted vectors for document ID: {document_id}")
            return True
        except Exception as e:
            logger.error(f"Error deleting vectors for document {document_id}: {str(e)}")
            return False
    
    async def get_document_stats(self) -> Dict[str, Any]:
        """Get statistics about the vector database."""
        try:
            collection = self.vector_store._collection
            count = collection.count()
            
            return {
                "total_documents": count,
                "collection_name": settings.CHROMA_COLLECTION_NAME,
                "embedding_model": "sentence-transformers/all-MiniLM-L6-v2"
            }
        except Exception as e:
            logger.error(f"Error getting document stats: {str(e)}")
            return {"error": str(e)}


# Global RAG service instance
rag_service = RAGService() 